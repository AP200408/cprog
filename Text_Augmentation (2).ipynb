{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q7XhqqqvNyg",
        "outputId": "b2bd289f-0672-4793-e658-3d13c6612ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.10/dist-packages (1.1.11)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.32.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.16.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Keyboard Augmentation***"
      ],
      "metadata": {
        "id": "zX1fqaqtA8cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "\n",
        "test_sentence_1 = \"I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\"\n",
        "\n",
        "aug = nac.KeyboardAug(name='Keyboard Aug', aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3,\n",
        "                      aug_word_min=1, aug_word_max=10, stopwords=None, tokenizer=None, reverse_tokenizer=None,\n",
        "                      include_special_char=True, include_numeric=True, include_upper_case=True, lang='en', verbose=0,\n",
        "                      stopwords_regex=None, model_path=None, min_char=4)\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_1)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_1)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNPKCVurwtwu",
        "outputId": "5d699664-db58-4f83-e8e3-06b3b8ccd775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went Shopping Today, and my trolly was with Bananas. I also had food at burgur palace\n",
            "['I went dhoplinh RoWay, and my trolly was wihG BwhaJas. I aks) had dpod at burgur palace']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for French language\n",
        "\n",
        "test_sentence_2 = \"comment vas-tu?\"\n",
        "aug = nac.KeyboardAug(name='Keyboard Aug', aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3,\n",
        "                      aug_word_min=1, aug_word_max=10, stopwords=None, tokenizer=None, reverse_tokenizer=None,\n",
        "                      include_special_char=True, include_numeric=True, include_upper_case=True, lang='fr', verbose=0,\n",
        "                      stopwords_regex=None, model_path=None, min_char=4)\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_2)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_2)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bscUS-smzlud",
        "outputId": "38868ea4-ce5b-4ab8-efa7-e50fe4dbb5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comment vas-tu?\n",
            "['commfH( vas - tu?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Optical Character Recognition(OCR) Augmentation***"
      ],
      "metadata": {
        "id": "tWq1Bjw-BJ0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug = nac.OcrAug(name='OCR_Aug',aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3,\n",
        "                 aug_word_min=1, aug_word_max=10, stopwords=None, tokenizer=None, reverse_tokenizer=None,\n",
        "                 verbose=0, stopwords_regex=None, min_char=1 )\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_1)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_1)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPsXRVMC2eNa",
        "outputId": "22f92534-575e-437e-b961-5299f7713911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went Shopping Today, and my trolly was with Bananas. I also had food at burgur palace\n",
            "['1 went Shopping Today, and my tr0l1y was with Bananas. I al80 had f0ud at burgur palace']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Random Character Augmentation***"
      ],
      "metadata": {
        "id": "sIssSiuHBWYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug = nac.RandomCharAug(action = 'substitute', name='RandomChar_Aug',aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3,\n",
        "                        aug_word_min=1, aug_word_max=10, include_upper_case = True, include_lower_case = True,\n",
        "                        include_numeric = True, min_char=4, swap_mode='adjacent', spec_char = '!@#$%^&*()_+',\n",
        "                        stopwords = None, tokenizer = None, reverse_tokenizer = None, verbose= 0, stopwords_regex=None, candidates= None )\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_1)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_1)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPJuq2kv-CQ2",
        "outputId": "296b8d82-27a7-46e9-c44b-6b90a2010f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went Shopping Today, and my trolly was with Bananas. I also had food at burgur palace\n",
            "['I went ShEpZin$ Today, and my trolly was !it* BvnIn7s. I alx! had fbSd at burgur QalaOe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug = nac.RandomCharAug(action = 'swap', name='RandomChar_Aug',aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3,\n",
        "                        aug_word_min=1, aug_word_max=10, include_upper_case = True, include_lower_case = True,\n",
        "                        include_numeric = True, min_char=4, swap_mode='adjacent', spec_char = '!@#$%^&*()_+',\n",
        "                        stopwords = None, tokenizer = None, reverse_tokenizer = None, verbose= 0, stopwords_regex=None, candidates= None )\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_1)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_1)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMeR0JYbAqn0",
        "outputId": "f4cf9b1e-65c4-43a6-d07c-b4df35a86dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went Shopping Today, and my trolly was with Bananas. I also had food at burgur palace\n",
            "['I wten Hsoppign Otady, and my trloyl was iwht Bananas. I also had food at burgur palcea']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug = nac.RandomCharAug(action = 'insert', name='RandomChar_Aug',aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3,\n",
        "                        aug_word_min=1, aug_word_max=10, include_upper_case = True, include_lower_case = True,\n",
        "                        include_numeric = True, min_char=4, swap_mode='adjacent', spec_char = '!@#$%^&*()_+',\n",
        "                        stopwords = None, tokenizer = None, reverse_tokenizer = None, verbose= 0, stopwords_regex=None, candidates= None )\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_1)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_1)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06QYDwkMC-lS",
        "outputId": "4edb783b-84e6-456b-a715-8f1ad5da1d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went Shopping Today, and my trolly was with Bananas. I also had food at burgur palace\n",
            "['I w9en7t Shopping Today, and my tr0olLly was wFivth Bananas. I also had qfOood at OburguBr pa0ljace']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug = nac.RandomCharAug(action = 'delete', name='RandomChar_Aug',aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3,\n",
        "                        aug_word_min=1, aug_word_max=10, include_upper_case = True, include_lower_case = True,\n",
        "                        include_numeric = True, min_char=4, swap_mode='adjacent', spec_char = '!@#$%^&*()_+',\n",
        "                        stopwords = None, tokenizer = None, reverse_tokenizer = None, verbose= 0, stopwords_regex=None, candidates= None )\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_1)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_1)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMXiog2wDB3x",
        "outputId": "ec50681e-0006-43b4-f157-bab0ba1d3f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went Shopping Today, and my trolly was with Bananas. I also had food at burgur palace\n",
            "['I nt Shopping Today, and my tlly was wi Bnas. I also had food at rgur plce']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Word Augmentation***"
      ],
      "metadata": {
        "id": "S5Q6GCp3F09f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Natural Language Toolkit (NLTK) is a Python library that's used to process natural language.\n",
        "# It's used by researchers, developers, and data scientists to: Analyze text data, Develop NLP applications, Support research and teaching in NLP and related areas, and Prototype and build research systems\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrmUQewfHGtO",
        "outputId": "3ecacbd2-7ba1-486f-8baa-6f394c691aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synonym Aug"
      ],
      "metadata": {
        "id": "GJGptXtOKnY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "test_sentence_3 = 'Augmenting text using NLP in Machine Learning'\n",
        "\n",
        "aug = naw.SynonymAug(aug_src = 'wordnet', model_path = None, name = 'Synonym_Aug', aug_min = 1, aug_max = 10, aug_p = 0.3,\n",
        "                     stopwords = None, tokenizer = None, reverse_tokenizer = None, verbose= 0, stopwords_regex=None)\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_3)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_3)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "id": "4jrndMn8DF_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "313335fa-ac3b-4538-e084-b0a1de8bc4f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmenting text using NLP in Machine Learning\n",
            "['Augmenting text apply NLP in Political machine Erudition']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Antonym Aug"
      ],
      "metadata": {
        "id": "R69o0K3WKuGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "test_sentence_4 = 'happy every one'\n",
        "\n",
        "aug = naw.AntonymAug(name = 'Antonym_Aug', aug_min = 1, aug_max = 10, aug_p = 0.3, lang = 'eng',\n",
        "                     stopwords = None, tokenizer = None, reverse_tokenizer = None, verbose= 0, stopwords_regex=None)\n",
        "# if we dont use any parameters then also it is okay.\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_4)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_4)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OQOwWB7IGsH",
        "outputId": "53d246ef-344d-4d28-9453-fc9b30486e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy every one\n",
            "['unhappy every one']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(naw.AntonymAug) # to check all info about a particular function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r159D45L_Rs",
        "outputId": "84228a1c-dfd8-4b30-959f-c2b1744c6514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class AntonymAug in module nlpaug.augmenter.word.antonym:\n",
            "\n",
            "class AntonymAug(nlpaug.augmenter.word.word_augmenter.WordAugmenter)\n",
            " |  AntonymAug(name='Antonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', stopwords=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n",
            " |  \n",
            " |  Augmenter that leverage semantic meaning to substitute word.\n",
            " |  \n",
            " |  :param str lang: Language of your text. Default value is 'eng'.\n",
            " |  :param float aug_p: Percentage of word will be augmented.\n",
            " |  :param int aug_min: Minimum number of word will be augmented.\n",
            " |  :param int aug_max: Maximum number of word will be augmented. If None is passed, number of augmentation is\n",
            " |      calculated via aup_p. If calculated result from aug_p is smaller than aug_max, will use calculated result from\n",
            " |      aug_p. Otherwise, using aug_max.\n",
            " |  :param list stopwords: List of words which will be skipped from augment operation.\n",
            " |  :param str stopwords_regex: Regular expression for matching words which will be skipped from augment operation.\n",
            " |  :param func tokenizer: Customize tokenization process\n",
            " |  :param func reverse_tokenizer: Customize reverse of tokenization process\n",
            " |  :param str name: Name of this augmenter\n",
            " |  \n",
            " |  >>> import nlpaug.augmenter.word as naw\n",
            " |  >>> aug = naw.AntonymAug()\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      AntonymAug\n",
            " |      nlpaug.augmenter.word.word_augmenter.WordAugmenter\n",
            " |      nlpaug.base_augmenter.Augmenter\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, name='Antonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', stopwords=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  get_candidates(self, tokens, token_idx)\n",
            " |  \n",
            " |  skip_aug(self, token_idxes, tokens)\n",
            " |  \n",
            " |  substitute(self, data)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  get_model(aug_src, lang) from builtins.type\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from nlpaug.augmenter.word.word_augmenter.WordAugmenter:\n",
            " |  \n",
            " |  align_capitalization(self, src_token, dest_token)\n",
            " |  \n",
            " |  is_stop_words(self, token)\n",
            " |  \n",
            " |  postprocess(self, data)\n",
            " |  \n",
            " |  pre_skip_aug(self, tokens, tuple_idx=None)\n",
            " |  \n",
            " |  preprocess(self, data)\n",
            " |  \n",
            " |  replace_reserve_word_by_stopword(self, text, reserve_word_aug, original_stopwords)\n",
            " |  \n",
            " |  replace_stopword_by_reserved_word(self, text, stopword_reg, reserve_word)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from nlpaug.augmenter.word.word_augmenter.WordAugmenter:\n",
            " |  \n",
            " |  clean(data) from builtins.type\n",
            " |  \n",
            " |  get_word_case(word) from builtins.type\n",
            " |  \n",
            " |  is_duplicate(dataset, data) from builtins.type\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from nlpaug.base_augmenter.Augmenter:\n",
            " |  \n",
            " |  __str__(self)\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  augment(self, data, n=1, num_thread=1)\n",
            " |      :param object/list data: Data for augmentation. It can be list of data (e.g. list \n",
            " |          of string or numpy) or single element (e.g. string or numpy). Numpy format only\n",
            " |          supports audio or spectrogram data. For text data, only support string or\n",
            " |          list of string.\n",
            " |      :param int n: Default is 1. Number of unique augmented output. Will be force to 1 \n",
            " |          if input is list of data\n",
            " |      :param int num_thread: Number of thread for data augmentation. Use this option \n",
            " |          when you are using CPU and n is larger than 1\n",
            " |      :return: Augmented data\n",
            " |      \n",
            " |      >>> augmented_data = aug.augment(data)\n",
            " |  \n",
            " |  crop(self, data)\n",
            " |  \n",
            " |  delete(self, data)\n",
            " |  \n",
            " |  evaluate(self)\n",
            " |  \n",
            " |  generate_aug_cnt(self, size, aug_p=None)\n",
            " |  \n",
            " |  generate_aug_idxes(self, inputs)\n",
            " |  \n",
            " |  insert(self, data)\n",
            " |  \n",
            " |  split(self, data)\n",
            " |  \n",
            " |  swap(self, data)\n",
            " |  \n",
            " |  tokenizer(self, tokens)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from nlpaug.base_augmenter.Augmenter:\n",
            " |  \n",
            " |  prob() from builtins.type\n",
            " |  \n",
            " |  sample(x, num=None) from builtins.type\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from nlpaug.base_augmenter.Augmenter:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Word Aug"
      ],
      "metadata": {
        "id": "Kcs1PxaAM4jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random : Augmenter that applies random word operations to textual input\n",
        "\n",
        "test_sentence_1 = \"I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\"\n",
        "\n",
        "aug = naw.RandomWordAug(action = 'delete', name='RandomWord_Aug',aug_min=1, aug_max=10, aug_p=0.3,\n",
        "                        stopwords = None, target_words=None, tokenizer = None, reverse_tokenizer = None, verbose= 0, stopwords_regex=None)\n",
        "\n",
        "# delete: deleting multiple words, but randomly\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_1)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_1)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wScA4IOnMCCE",
        "outputId": "7447dd06-8a9c-4d11-d101-68b06e6c7088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went Shopping Today, and my trolly was with Bananas. I also had food at burgur palace\n",
            "['I went Shopping Today, and with Bananas. also food at burgur']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence_1 = \"I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\"\n",
        "\n",
        "aug = naw.RandomWordAug(action = 'substitute', name='RandomWord_Aug',aug_min=1, aug_max=10, aug_p=0.3,\n",
        "                        stopwords = None, target_words=None, tokenizer = None, reverse_tokenizer = None, verbose= 0, stopwords_regex=None)\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_1)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_1)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqVHm78XOWzS",
        "outputId": "ade10b2d-c6b9-4cd8-95a5-04c5743a2e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went Shopping Today, and my trolly was with Bananas. I also had food at burgur palace\n",
            "['I went Shopping Today, _ my trolly was _ Bananas. _ also had food _ _ _']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence_1 = \"I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\"\n",
        "\n",
        "aug = naw.RandomWordAug(action = 'crop', name='RandomWord_Aug',aug_min=1, aug_max=10, aug_p=0.3,\n",
        "                        stopwords = None, target_words=None, tokenizer = None, reverse_tokenizer = None, verbose= 0, stopwords_regex=None)\n",
        "# crop: deletes multiple words in a continuous sequence.\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_1)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_1)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGHwfNj5OziB",
        "outputId": "bfbd01c3-93ae-4ba6-bd7e-99fe639555b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
            "['I went Shopping Today, and my trolly also had food at burgur palace']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence_1 = \"I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\"\n",
        "\n",
        "aug = naw.RandomWordAug(action = 'swap', name='RandomWord_Aug',aug_min=1, aug_max=10, aug_p=0.3,\n",
        "                        stopwords = None, target_words=None, tokenizer = None, reverse_tokenizer = None, verbose= 0, stopwords_regex=None)\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_1)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_1)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sHsZpezO9gC",
        "outputId": "937a8e7f-ced4-4dcd-bb4a-1cafce73b698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
            "['Went I Today Shopping, and my was with trolly Bananas filled. I also had food at burgur palace']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spelling Augmenter"
      ],
      "metadata": {
        "id": "ctiEO9dMQa1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spelling Augmenter is used to introduce intentional spelling mistakes or variations into text data\n",
        "\n",
        "test_sentence_1 = \"I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\"\n",
        "\n",
        "aug = naw.SpellingAug(dict_path = None, name='Spelling_Aug',aug_min=1, aug_max=10, aug_p=0.3,\n",
        "                        stopwords = None, tokenizer = None, reverse_tokenizer = None, include_reverse=True,verbose= 0, stopwords_regex=None)\n",
        "# spelling aug changes the spelling of the words, be it meaningful or not\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_1)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_1)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh-hJyklPVjy",
        "outputId": "286d69b4-7429-463e-d40a-958af566a6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
            "['I went Shopping Today, ahd Mys trolly wa filled with Bananas. I also have fooh at burgur palce']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Augmentation"
      ],
      "metadata": {
        "id": "NZQDk6ZeReGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Augmentation is a technique used in NLP to modify text data by splitting words or phrases\n",
        "\n",
        "test_sentence_1 = \"I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\"\n",
        "\n",
        "aug = naw.SplitAug(name='Split_Aug',aug_min=1, aug_max=10, min_char=4, aug_p = 0.3,\n",
        "                        stopwords = None, tokenizer = None, reverse_tokenizer = None, verbose= 0, stopwords_regex=None)\n",
        "# Augmenter that applies word splitting operation to textual input\n",
        "\n",
        "test_sentence_aug = aug.augment((test_sentence_1)) #if we put a comma and add a number, say 2, it will augmenting the sentence 2 times\n",
        "print(test_sentence_1)\n",
        "print(test_sentence_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5xOD8qsQzjQ",
        "outputId": "8ad99bb6-7c55-4a9b-b300-6808fe5beb0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
            "['I went Shopping T oday, and my trolly was fil led wi th Bananas. I al so had f ood at b urgur palace']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***TF-IDF Vectorizer***"
      ],
      "metadata": {
        "id": "WnPcS0kKZgP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "text = ['This is the first document.', #d0\n",
        "        'This document is the second document.', #d1\n",
        "        'And this is the third one.', #d2\n",
        "        'Is this the first document?'] #d3\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(text) # document_index     word_index      tfidf_values\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(tfidf_matrix)\n",
        "\n",
        "print(tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMnnfdS5SZ8v",
        "outputId": "50146bda-e5c6-45de-e570-6d0c0223f4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "  (0, 8)\t0.38408524091481483\n",
            "  (0, 3)\t0.38408524091481483\n",
            "  (0, 6)\t0.38408524091481483\n",
            "  (0, 2)\t0.5802858236844359\n",
            "  (0, 1)\t0.46979138557992045\n",
            "  (1, 8)\t0.281088674033753\n",
            "  (1, 3)\t0.281088674033753\n",
            "  (1, 6)\t0.281088674033753\n",
            "  (1, 1)\t0.6876235979836938\n",
            "  (1, 5)\t0.5386476208856763\n",
            "  (2, 8)\t0.267103787642168\n",
            "  (2, 3)\t0.267103787642168\n",
            "  (2, 6)\t0.267103787642168\n",
            "  (2, 0)\t0.511848512707169\n",
            "  (2, 7)\t0.511848512707169\n",
            "  (2, 4)\t0.511848512707169\n",
            "  (3, 8)\t0.38408524091481483\n",
            "  (3, 3)\t0.38408524091481483\n",
            "  (3, 6)\t0.38408524091481483\n",
            "  (3, 2)\t0.5802858236844359\n",
            "  (3, 1)\t0.46979138557992045\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = ['This is the first document.',\n",
        "          'This document is the second document.']\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "new_document = ['This is a new document.']\n",
        "\n",
        "tfidf_vector = vectorizer.transform(new_document)\n",
        "\n",
        "print(tfidf_vector) # 'this' 'is' and 'document' are three words that are common in \"new_document\" and in \"corpus\"\n",
        "print()\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print()\n",
        "print(tfidf_vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Voo-4_nqa0B6",
        "outputId": "6673108f-75c0-4680-b980-eb2e47fc9afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t0.5773502691896258\n",
            "  (0, 2)\t0.5773502691896258\n",
            "  (0, 5)\t0.5773502691896258\n",
            "\n",
            "['document' 'first' 'is' 'second' 'the' 'this']\n",
            "\n",
            "[[0.57735027 0.         0.57735027 0.         0.         0.57735027]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "test_sentence = \"I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\"\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([test_sentence])\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "\n",
        "def tfidf_aug_specific(text, top_n=3, aug_p=0.3):\n",
        "    tfidf_matrix = vectorizer.transform([text])\n",
        "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "    sorted_indices = np.argsort(tfidf_scores)[::-1]\n",
        "    important_words = [feature_names[i] for i in sorted_indices[:top_n]]\n",
        "\n",
        "    aug = naw.SynonymAug(aug_src='wordnet', aug_p=aug_p)\n",
        "    augmented_text = aug.augment(text)\n",
        "\n",
        "    return augmented_text\n",
        "\n",
        "augmented_sentence = tfidf_aug_specific(test_sentence)\n",
        "\n",
        "print(\"Original Sentence:\", test_sentence)\n",
        "print(\"Augmented Sentence:\", augmented_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meUlPW9SinaV",
        "outputId": "3d5b5ab0-abb5-4635-a788-792b85120f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
            "Augmented Sentence: ['Atomic number 53 went Snitch Today, and my trolly was filled with Banana tree. I likewise had food at burgur palace']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4FNh4QRrnl8S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}